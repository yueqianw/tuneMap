# -*- coding: utf-8 -*-
"""Music_Generate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zisYxNNq_i3MAvivm_muMh7HR6H2pW0J
"""

# ---------------------------------------------------------
# Dependencies:
#   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117  # or cpu-only
#   pip install transformers soundfile pillow geopy git+https://github.com/facebookresearch/encodec.git
#   pip install geopy
# ---------------------------------------------------------
import os
import torch
import numpy as np
import torchaudio
import soundfile as sf
from PIL import Image
from geopy.geocoders import Nominatim
from transformers import (
    BlipProcessor, BlipForConditionalGeneration,
    CLIPProcessor, CLIPModel,
    pipeline as hf_pipeline
)
from typing import List, Optional, Tuple

print("DEBUG_MUSICGEN: (A) import dependencies done.")

# 设备配置
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device_idx = 0 if device=='cuda' else -1
print(f"Device set to use {device_idx}") # 打印出实际使用的设备

# 模型初始化
print("DEBUG_MUSICGEN: (B) model initialization ...")
_blip_processor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')
_blip_model = BlipForConditionalGeneration.from_pretrained(
    'Salesforce/blip-image-captioning-base'
).to(device)

print("DEBUG_MUSICGEN: (C) _blip_processor _blip_model done.")
_clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')
_clip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32').to(device)

print("DEBUG_MUSICGEN: (D) _clip_processor _clip_model done.")
_refiner = hf_pipeline('text2text-generation', model='google/flan-t5-small', device=device_idx)

print("DEBUG_MUSICGEN: (E) _refiner done.")
_music_pipe = hf_pipeline('text-to-audio', model='facebook/musicgen-small', device=device_idx)

print("DEBUG_MUSICGEN: (F) _music_pipe done.")
_geolocator = Nominatim(user_agent='image2music_module')

print("DEBUG_MUSICGEN: (D) _geolocator done.")
_STYLE_MAP = {
    'joyful':    {'tempo':130,'instrument':'bright guitar and piano','dynamics':'lively','rhythm':'upbeat syncopation'},
    'melancholic':{'tempo':55,'instrument':'solo cello and pads','dynamics':'soft','rhythm':'slow flowing patterns'},
    'epic':      {'tempo':95,'instrument':'brass and timpani','dynamics':'powerful','rhythm':'strong pulse'},
    'electronic':{'tempo':128,'instrument':'synth bass and drums','dynamics':'energetic','rhythm':'steady four-on-the-floor'},
    'serene':    {'tempo':70,'instrument':'harp and chimes','dynamics':'gentle','rhythm':'spacious patterns'},
    'mysterious':{'tempo':65,'instrument':'strings and pads','dynamics':'subdued','rhythm':'irregular motifs'},
    'nostalgic': {'tempo':60,'instrument':'vintage piano and strings','dynamics':'warm','rhythm':'gentle groove'},
    'dramatic':  {'tempo':100,'instrument':'strings and horns','dynamics':'intense','rhythm':'accented rhythms'},
    'romantic':  {'tempo':85,'instrument':'strings and violin','dynamics':'expressive','rhythm':'waltz-like patterns'},
    'dark':      {'tempo':50,'instrument':'synths and bass','dynamics':'brooding','rhythm':'off-beat pulse'},
    'uplifting': {'tempo':120,'instrument':'brass and choirs','dynamics':'bright','rhythm':'energetic pulse'},
    'ambient':   {'tempo':60,'instrument':'pads and drones','dynamics':'ethereal','rhythm':'arrhythmic textures'}
}

print("DEBUG_MUSICGEN: (E) _STYLE_MAP done.")
# 工具函数
def _ensure_stereo_44k(wave: torch.Tensor, sr: int, target_sr: int=44100) -> Tuple[torch.Tensor,int]:
    if wave.ndim==1:
        wave=wave.unsqueeze(0)
    elif wave.shape[0]>wave.shape[-1]:
        wave=wave.T.contiguous()
    if wave.shape[0]==1:
        wave=wave.repeat(2,1)
    if sr!=target_sr:
        wave=torchaudio.functional.resample(wave, sr, target_sr)
        sr=target_sr
    return torch.clamp(wave, -1.0, 1.0), sr

print("DEBUG_MUSICGEN: (F) _ensure_stereo_44k done.")
# 核心函数
def generate_music(
    image_paths: List[str],
    coords: Tuple[float,float],
    output_wav: str,
    style_override: Optional[str]=None,
    refine_description: bool=True,
    caption_max_tokens: int=10,
    refine_max_tokens: int=10,
    music_max_tokens: int=500
) -> str:
    """
    根据多张图片与地理坐标生成音乐。

    参数:
      image_paths: 图片文件路径列表
      coords: (lat, lon)
      output_wav: 输出 WAV 文件路径
      style_override: 可选，指定一个风格标签
      refine_description: 是否精炼场景描述
    返回:
      output_wav 路径
    """
    lat, lon = coords
    loc = _geolocator.reverse((lat, lon), language='en')
    location_str = loc.address if loc else f'Lat {lat}, Lon {lon}'

    descriptions, styles = [], []
    for path in image_paths:
        img = Image.open(path).convert('RGB')
        inputs = _blip_processor(images=img, return_tensors='pt').to(device)
        out = _blip_model.generate(**inputs, max_new_tokens=caption_max_tokens)
        base = _blip_processor.decode(out[0], skip_special_tokens=True)
        if refine_description:
            prompt = f"Expand this caption into a vivid scene description: {base}"
            res = _refiner(prompt, max_length=refine_max_tokens, do_sample=False)[0]['generated_text']
            if ':' in res:
                res = res.split(':',1)[-1].strip()
            descriptions.append(res)
        else:
            descriptions.append(base)
        if style_override in _STYLE_MAP:
            styles.append(style_override)
        else:
            clip_in = _clip_processor(images=img, text=list(_STYLE_MAP.keys()), return_tensors='pt', padding=True).to(device)
            clip_out = _clip_model(**clip_in)
            idx = int(torch.argmax(clip_out.logits_per_image[0]))
            styles.append(list(_STYLE_MAP.keys())[idx])

    main_style = style_override if style_override in _STYLE_MAP else max(set(styles), key=styles.count)
    raw_desc = ' '.join([d.rstrip('.') + '.' for d in descriptions])
    details = _STYLE_MAP.get(main_style, {'tempo':80,'instrument':'piano','dynamics':'moderate','rhythm':'steady'})

    prompt = (
        f"Walking through {location_str} in a {main_style} mood, I see: {raw_desc} "
        f"Compose a piece at {details['tempo']} BPM, featuring {details['instrument']}, "
        f"with {details['dynamics']} dynamics and {details['rhythm']}, dynamic crescendos, and ambient harmonies."
    )
    result = _music_pipe(prompt, forward_params={'max_new_tokens': music_max_tokens})
    if isinstance(result, list): result = result[0]
    wave, sr = result['audio'], result['sampling_rate']
    wave = torch.from_numpy(wave) if isinstance(wave, np.ndarray) else torch.tensor(wave)
    if wave.ndim==3: wave=wave[0]
    wave, sr = _ensure_stereo_44k(wave, sr)
    os.makedirs(os.path.dirname(output_wav) or '.', exist_ok=True)
    sf.write(output_wav, wave.cpu().numpy().T, sr, subtype='PCM_16')
    return output_wav

# 调用示例
if __name__ == '__main__':
    # 示例：目录转列表
    import glob
    img_dir = 'D:/semester_1_year_1/CPAC/music-map/public/images'
    imgs = glob.glob(os.path.join(img_dir, '*.jpg')) + glob.glob(os.path.join(img_dir, '*.png'))
    out_file = 'D:/semester_1_year_1/CPAC/music-map/generated_output.wav'
    wav = generate_music(
        image_paths=imgs,
        coords=(41.9028, 12.4964),
        output_wav=out_file,
        style_override=None,
        refine_description=True
    )
    print('Generated music saved at:', wav)