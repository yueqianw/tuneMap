# -*- coding: utf-8 -*-
"""Music_Generate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zisYxNNq_i3MAvivm_muMh7HR6H2pW0J
"""

# ---------------------------------------------------------
# Dependencies:
#   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117  # or cpu-only
#   pip install transformers soundfile pillow geopy git+https://github.com/facebookresearch/encodec.git
#   pip install geopy
# ---------------------------------------------------------
# =====================  Imports  =========================
import os, warnings, importlib, re, numpy as np
from typing import List, Tuple

# -- optional wikipedia
try:
    import wikipedia
except ModuleNotFoundError:
    wikipedia = None

import torch, torchaudio, soundfile as sf
from PIL import Image
from geopy.geocoders import Nominatim
from transformers import (
    BitsAndBytesConfig, AutoTokenizer, AutoModelForSeq2SeqLM,
    BlipProcessor, BlipForConditionalGeneration,
    CLIPProcessor, CLIPModel,
    pipeline as hf_pipeline
)

# =====================  Global config  ===================
device = "cuda" if torch.cuda.is_available() else "cpu"
QUANTIZE_8BIT_DEFAULT = True
USE_FAST_TOKENIZER = False

# =====================  Loader (auto-fallback) ============
def _load_model(name: str, enable_8bit=QUANTIZE_8BIT_DEFAULT):
    tok = AutoTokenizer.from_pretrained(name, use_fast=USE_FAST_TOKENIZER)
    qc = None
    if enable_8bit and device == "cuda":
        try:
            importlib.import_module("bitsandbytes")
            importlib.import_module("triton.ops")
            qc = BitsAndBytesConfig(load_in_8bit=True)
        except ModuleNotFoundError as e:
            warnings.warn(f"[量化回退] {e} → 全精度加载", RuntimeWarning)

    model = AutoModelForSeq2SeqLM.from_pretrained(
        name,
        quantization_config=qc,
        device_map="auto" if qc else None,
        torch_dtype=torch.float16 if qc else None
    ).eval()
    return model, tok

# =====================  Model init  ======================
blip_proc  = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
blip_model = BlipForConditionalGeneration.from_pretrained(
    "Salesforce/blip-image-captioning-base",
    torch_dtype=torch.float16 if device == "cuda" else None
).to(device).eval()

clip_proc  = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
clip_model = CLIPModel.from_pretrained(
    "openai/clip-vit-base-patch32",
    torch_dtype=torch.float16 if device == "cuda" else None
).to(device).eval()

STYLE_CANDS = ["ambient","minimalist","uplifting","epic","joyful",
               "melancholic","nostalgic","dramatic","electronic","folk"]

_refiner,_ref_tok = _load_model("google/flan-t5-small")
refiner_pipe = hf_pipeline("text2text-generation", model=_refiner,
                           tokenizer=_ref_tok, device=0 if device=="cuda" else -1)

music_pipe = hf_pipeline("text-to-audio", model="facebook/musicgen-small",
                         device=0 if device=="cuda" else -1)

geolocator = Nominatim(user_agent="image2music_module")

# =====================  Smart scale detect  ==============
KEYWORDS = {
    r"\bmaqam\b":"Arabic maqam", r"\bmakam\b":"Turkish makam",
    r"\bdastgah\b":"Persian dastgah", r"\braga\b":"Hindustani raga",
    r"\bpelog\b":"Gamelan pelog", r"\bslendro\b":"Gamelan slendro",
    r"\bpentatonic\b":"pentatonic scale", r"\bphrygian\b":"phrygian mode",
    r"\bmixolydian\b":"mixolydian mode", r"\bdorian\b":"dorian mode",
}

def smart_traditional_mode(country:str)->str|None:
    if not country: return None
    # ① Wikipedia
    if wikipedia is not None:
        try:
            for title in (f"Music of {country}",f"Traditional music of {country}"):
                page = wikipedia.page(title, auto_suggest=False)
                text = page.summary.lower()
                for pat,desc in KEYWORDS.items():
                    if re.search(pat,text):
                        print(f"[Smart-Mode] Wiki 捕获 «{desc}»")
                        return desc
        except Exception:
            pass
    else:
        print("[Smart-Mode] wikipedia 模块缺失，跳过 Wiki 查询")

    # ② LLM
    try:
        q=f"Name one traditional musical scale or mode used in the folk music of {country}. Answer with a short phrase only."
        ans=refiner_pipe(q,max_length=20)[0]["generated_text"].strip()
        if 3<=len(ans)<=40:
            print(f"[Smart-Mode] LLM 建议 «{ans}»")
            return ans
    except Exception:
        pass
    return None

# =====================  Visual cues  =====================
def visual_cues(img:Image.Image)->dict:
    hsv=img.convert("HSV")
    h,s,v=[np.array(c).astype(np.float32)/255. for c in hsv.split()]
    warm=(h.mean()<.25) or (h.mean()>.85)
    key_mode="major" if warm else "minor"
    light=v.mean()
    dynamic="very bright"if light>.75 else"bright"if light>.55 else"balanced"if light>.35 else"soft"
    ed=np.hypot(*np.gradient(np.array(img.convert("L"),dtype=np.float32)/255.)).mean()
    tempo="fast"if ed>.12 else"moderate"if ed>.07 else"slow"
    return dict(key_mode=key_mode,dynamic=dynamic,tempo=tempo)

# =====================  Audio helper  ====================
def stereo_44k(audio:torch.Tensor,sr:int):
    if audio.ndim==1: audio=audio.unsqueeze(0)
    if audio.shape[0]==1: audio=audio.repeat(2,1)
    if sr!=44100: audio=torchaudio.functional.resample(audio,sr,44100); sr=44100
    return torch.clamp(audio,-1,1),sr

# =====================  Main =============================
def generate_music(
    image_paths:List[str],
    coords:Tuple[float,float],
    output_wav:str,
    style_hint:str="",
    refine_description:bool=False,
    duration_sec:int=30
)->str:

    if not image_paths: raise ValueError("image_paths 不能为空")
    for p in image_paths:
        if not os.path.isfile(p): raise FileNotFoundError(p)

    # 地理
    lat,lon=coords
    loc=geolocator.reverse((lat,lon),language="en")
    addr=loc.raw["address"] if loc and"address"in loc.raw else {}
    country=addr.get("country","")
    region_label=", ".join(dict.fromkeys([addr.get(k,"") for k in("city","state","country")])).strip(", ") \
                 or f"Lat {lat:.2f}, Lon {lon:.2f}"

    # 图像描述 & cues
    imgs,descs,cues=[],[],[]
    for p in image_paths:
        img=Image.open(p).convert("RGB").resize((224,224))
        imgs.append(img); cues.append(visual_cues(img))

    with torch.no_grad():
        outs=blip_model.generate(**blip_proc(images=imgs,return_tensors="pt").to(device),
                                 max_new_tokens=30)

    for i,img in enumerate(imgs):
        cap=blip_proc.decode(outs[i],skip_special_tokens=True)
        if refine_description:
            cap=refiner_pipe(f"Expand: {cap}",max_length=60)[0]["generated_text"].split(":",1)[-1].strip()
        descs.append(cap.rstrip(".")+".")

    key_mode=max(set(c["key_mode"] for c in cues),key=[c["key_mode"]for c in cues].count)
    dynamic=max(set(c["dynamic"]for c in cues),key=[c["dynamic"]for c in cues].count)
    tempo  =max(set(c["tempo"]  for c in cues),key=[c["tempo"]  for c in cues].count)

    trad_mode=smart_traditional_mode(country)
    mode_phrase=f"in the traditional {trad_mode} mode" if trad_mode else f"in a {key_mode} tonality"

    # 风格
    style=style_hint.strip()
    if not style:
        with torch.no_grad():
            score=torch.zeros(len(STYLE_CANDS),device=device)
            for img in imgs:
                inp=clip_proc(images=img,text=STYLE_CANDS,return_tensors="pt",padding=True).to(device)
                score+=clip_model(**inp).logits_per_image[0]
            style=STYLE_CANDS[int(score.argmax())]
        print(f"[Auto-Style] {style}")

    # Prompt
    prompt=(f"You are composing {style} music inspired by the locale of {region_label}. "
            f"The piece should be {mode_phrase}, feature {dynamic} dynamics, and keep a {tempo} tempo. "
            f"It must use traditional instruments commonly found in this region. "
            f"Visual inspiration: {' '.join(descs)} "
            f"Please produce a coherent, high-quality musical piece about {duration_sec} seconds long.")

    print("\n=== MUSICGEN PROMPT ===\n",prompt,"\n=======================\n")

    # MusicGen
    tokens=duration_sec*50
    with torch.no_grad():
        out=music_pipe(prompt,forward_params={"max_new_tokens":tokens})
    res=out[0] if isinstance(out,list) else out
    audio,sr=res["audio"],res["sampling_rate"]
    audio=torch.tensor(audio) if not isinstance(audio,torch.Tensor) else audio
    if audio.ndim==3: audio=audio[0]
    audio,sr=stereo_44k(audio,sr)
    os.makedirs(os.path.dirname(output_wav)or".",exist_ok=True)
    sf.write(output_wav,audio.cpu().numpy().T,sr,subtype="PCM_16")
    print(f"[✓] 生成：{output_wav}")
    return output_wav

# 调用示例
if __name__ == '__main__':
    # 示例：目录转列表
    import glob
    img_dir = 'D:/semester_1_year_1/CPAC/music-map/public/images'
    imgs = glob.glob(os.path.join(img_dir, '*.jpg')) + glob.glob(os.path.join(img_dir, '*.png'))
    out_file = 'D:/semester_1_year_1/CPAC/music-map/generated_output.wav'
    wav = generate_music(
        image_paths=imgs,
        coords=(41.9028, 12.4964),
        output_wav=out_file,
        style_override=None,
        refine_description=True
    )
    print('Generated music saved at:', wav)